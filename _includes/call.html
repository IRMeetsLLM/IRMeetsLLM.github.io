<section id="call" class={{include.class}}>
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading">Call for Papers</h2>
        <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
      </div>
    </div>
    <div class="row text-justify">
      <div class="col-md-12">
        <p class="large text-muted">
            At the heart of the "Information Retrieval Meets Large Language Models Workshop" lies the ambition to pioneer research that bridges the gap between information retrieval and large language models (LLMs). This workshop is dedicated to exploring how LLMs can enhance information retrieval algorithms, introducing a new era of data processing and analysis. We aim to delve into the potential of generative models, particularly in creating AI-generated content (AIGC), to supplement and diversify the information available, catering to a broader array of user preferences and information needs. A significant focus will be on the transformative potential of LLMs in reshaping user interactions with information retrieval systems, harnessing the latest advancements in conversational AI and user experience design. In parallel, the workshop will address the critical issues of trust and ethics in AI-driven information retrieval. This involves scrutinizing content authenticity, mitigating algorithmic biases, and ensuring adherence to evolving ethical and legal standards.       </p>
        <p class="large text-muted">
            Furthermore, the workshop endeavors to promote the development and adoption of innovative evaluation methodologies. These new approaches, including advanced metrics and human-centered evaluation techniques, are essential for assessing the effectiveness and impact of LLM-enhanced information retrieval systems. Through these multifaceted objectives, the workshop aspires to set a new benchmark in the integration of information retrieval and large language models, paving the way for future innovations in the field.
        </p>  
        <div class="col-lg-12 text-center">
        <h3 class="section-subheading">Topics of Interest</h3>
        </div>
        <ul class="large text-muted">
    <li><strong>LLMs in Query Understanding and Reformulation:</strong>
        <ul>
            <li>Exploring the use of LLMs for interpreting and rephrasing ambiguous queries, including query expansion with semantic understanding and contextual query reformulation.</li>
        </ul>
    </li>
    <li><strong>LLMs in Understanding User Behavior:</strong>
        <ul>
            <li>Utilizing LLMs to predict user satisfaction in search sessions and personalize search results based on analysis of historical user data.</li>
        </ul>
    </li>
    <li><strong>Personalized Search Techniques Using LLMs:</strong>
        <ul>
            <li>Developing user profiles with the aid of LLMs to improve search relevance and constructing personalized knowledge graphs.</li>
        </ul>
    </li>
    <li><strong>Conversational Search Powered by LLMs:</strong>
        <ul>
            <li>Innovations in dialogue systems for search and continuous learning from user interactions in conversational information retrieval.</li>
        </ul>
    </li>
    <li><strong>LLM-driven Indexing Strategies:</strong>
        <ul>
            <li>Implementing LLM-based models for generative retrieval, index pruning, optimization, and creating abstract document representations.</li>
        </ul>
    </li>
    <li><strong>Ranking and Matching with LLMs:</strong>
        <ul>
            <li>Using LLMs for contextual ranking, semantic query-document matching, and multi-modal search result ranking.</li>
        </ul>
    </li>
    <li><strong>LLMs in Evaluation Metrics for Information Retrieval:</strong>
        <ul>
            <li>Developing new IR evaluation metrics leveraging LLM language understanding, automating relevance judgment, and emulating user satisfaction testing.</li>
        </ul>
    </li>
    <li><strong>Data Augmentation for IR with LLMs:</strong>
        <ul>
            <li>Generating synthetic queries and enhancing IR corpora diversity using LLM-generated content.</li>
        </ul>
    </li>
    <li><strong>Incorporating IR Techniques in LLM Pre-training:</strong>
        <ul>
            <li>Merging traditional IR methods with LLM pre-training for domain adaptation, retrieval-enhanced strategies, and impact analysis.</li>
        </ul>
    </li>
    <li><strong>Retrieval Adapters for Enhancing LLMs:</strong>
        <ul>
            <li>Creating modular retrieval adapters for specific IR tasks and customizable IR features within LLMs, improving transfer learning.</li>
        </ul>
    </li>
    <li><strong>Knowledge-Enriched LLMs for IR:</strong>
        <ul>
            <li>Integrating external knowledge bases with LLMs, using IR for real-time data feeding, and enhancing factual accuracy with dynamic retrieval.</li>
        </ul>
    </li>
    <li><strong>Retrieval Augmented Generation for LLMs:</strong>
        <ul>
            <li>Leveraging document retrieval to enrich LLM responses, comparing RAG with end-to-end models, and examining complex reasoning strategies.</li>
        </ul>
    </li>
    <li><strong>Hybrid Models of LLMs and Classic IR:</strong>
        <ul>
            <li>Evaluating hybrid models in specialized domains, enhancing classic IR models' features with LLMs, and maintaining system interpretability.</li>
        </ul>
    </li>
    <li><strong>Training and Reasoning Strategies for LLMs in IR:</strong>
        <ul>
            <li>Implementing feedback loops, multi-task learning, meta-learning, few-shot learning, explainable AI, transfer learning, and scalability in LLM training for IR.</li>
        </ul>
    </li>
    <li><strong>Extensions in Multi-Lingual and Multi-Modal Scenarios:</strong>
        <ul>
            <li>Investigating LLMs in cross-lingual retrieval, enhancing multi-lingual corpora, interpreting and indexing multi-modal data, and integrating LLMs with other modalities for unified search platforms.</li>
        </ul>
    </li>

        </ul>
        <div class="col-lg-12 text-center">
        <h3 class="section-subheading">Submission guidelines</h3>
        </div>
        <p class="large text-muted">

            Submissions must be in a single PDF file, formatted according to the ACM WWW 2024 template. Papers may range from 4 to 8 pages, with additional unlimited pages for references. Authors can choose the length of their paper, as no distinction will be made between long and short papers. All submissions will undergo a "double-blind" review process, evaluated for their relevance, scientific novelty, and technical quality by expert reviewers.        </p>

        <p class="large text-muted">
        Submission site: <a href="https://easychair.org/conferences/?conf=thewebconf2024_workshops">https://easychair.org/conferences/?conf=thewebconf2024_workshops</a>.
        </p>
        <div class="col-lg-12 text-center">
        <h3 class="section-subheading">Important Dates</h3>
        </div>
        <ul class="large text-muted">
        <p>
          <strong>Paper Submission Deadline:</strong> February 5, 2024
        </p>
        <p>
          <strong>Acceptance Notification:</strong> March 4, 2024
        </p>
        <p>
          <strong>Workshop Date:</strong> May 13, 2024
        </p>
        </ul>
      </div>
    </div>
  </div>
</section>
